{
  "name": "Random Forests and aggregation methods",
  "tagline": "This projects aggregates a Bagging, Random Forest and MLP classifier for data competition",
  "body": "## Random Forests and aggregation methods\r\nThis post focuses on aggregation methods. Random Forest and Bootstrap Aggregation are by themselves instances of aggregation methods. In line of a data competition, we have team members make a classifier each and aggregate them to a final submission.\r\n\r\n## Data\r\nThe data is part of a class competition hosted by professor Alexandre Thiery for the Data mining course at NUS. The data consists of 20.000 labeled samples for training and 20.000 unlabeled samples for evaluation. Targets are seven classes. The training data is split in *train_data.csv*, *test_data.csv* and *assembling_data.csv* for our training purposes.\r\n\r\n## Ensemble\r\nThe main script is *ensemble.py*, which takes in the distribution over the classes for each sample in both the *assembling_data.csv* and the final test data. \r\nThe individual classifiers with the accuracies for the labeled data\r\n  *  Naive Bayes classfier: 77%\r\n  *  Random Forest (with 22 covariates taken for each split): 95.8%\r\n  *  Bagging: 96.4% \r\n  *  Neural Network (1 hidden layer, 115 neurons): 96.4%  \r\n  *  Multi-class one-vs-one LASSO: 92.8%\r\n  *  Multi-class one-vs-one RIDGE: $93.5 %$\r\n\r\nCode for the individual classifiers in *baggin_main.m*, *random_forest_main.r* (author: Mareva Brixy) and *MLP_1_hidden.py*. \r\n\r\n## Aggregation\r\nFor the aggregation, we take a naive weighted average. The weights are the accuracies as obtained on the *assembling_data.csv*. With some selection, we obtain 96.6% accuracy by combining the Random Forest, Bagging and Neural network. Already, this naive weighted average as an ensemble outperforms the individual accuracies.\r\n\r\n## Output\r\nA typical output of the script would be like this\r\n![example_output](https://github.com/RobRomijnders/trees_ensemble/blob/master/output_example.png?raw=true)\r\nPer classifier, you will get the confusion matrix, the accuracy on the labeled data and the average confidence per misclassified sample. Moreover, you will get the ensemble accuracy and metrics for the unlabeled dataset.\r\n\r\nAs always, I am curious to any comments and questions. Reach me at romijndersrob@gmail.com\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}